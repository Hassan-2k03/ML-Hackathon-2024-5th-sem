{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1000, 11)\n",
      "Test data shape: (100, 10)\n",
      "\n",
      "Sample training data:\n",
      "   Dialogue_ID  Utterance_ID Sentiment                     video_clip_path\n",
      "0            0             3   neutral  Set_3\\\\train\\\\videos\\dia0_utt3.mp4\n",
      "1            1             3  negative  Set_3\\\\train\\\\videos\\dia1_utt3.mp4\n",
      "2            3             3   neutral  Set_3\\\\train\\\\videos\\dia3_utt3.mp4\n",
      "3            4             6   neutral  Set_3\\\\train\\\\videos\\dia4_utt6.mp4\n",
      "4            6             3   neutral  Set_3\\\\train\\\\videos\\dia6_utt3.mp4\n",
      "\n",
      "Sentiment distribution in training data:\n",
      "Sentiment\n",
      "neutral     473\n",
      "negative    294\n",
      "positive    233\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import librosa\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import VideoFileClip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load training and test data\n",
    "train_df = pd.read_csv(r'Set_3\\\\train\\\\text.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(r'Set_3\\\\test\\\\text.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define video directories\n",
    "train_video_dir = r'Set_3\\\\train\\\\videos'\n",
    "test_video_dir = r'Set_3\\\\test\\\\videos'\n",
    "\n",
    "def get_video_clip_path(row, video_dir):\n",
    "    dialogue_id = row['Dialogue_ID']\n",
    "    utterance_id = row['Utterance_ID']\n",
    "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    return os.path.join(video_dir, filename)\n",
    "\n",
    "# Add video paths to dataframes\n",
    "train_df['video_clip_path'] = train_df.apply(lambda x: get_video_clip_path(x, train_video_dir), axis=1)\n",
    "test_df['video_clip_path'] = test_df.apply(lambda x: get_video_clip_path(x, test_video_dir), axis=1)\n",
    "\n",
    "# Verify data loading\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nSample training data:\")\n",
    "print(train_df[['Dialogue_ID', 'Utterance_ID', 'Sentiment', 'video_clip_path']].head())\n",
    "print(\"\\nSentiment distribution in training data:\")\n",
    "print(train_df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "class MultimodalFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # Initialize BERT tokenizer and model for text features\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_video_features(self, video_path, num_frames=20):\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            frame_indices = np.linspace(0, frame_count-1, num_frames, dtype=int)\n",
    "            \n",
    "            for idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame = cv2.resize(frame, (224, 224))\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    frames.append(frame)\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            if frames:\n",
    "                frames = np.stack(frames)\n",
    "                features = {\n",
    "                    'video_mean': frames.mean(),\n",
    "                    'video_std': frames.std(),\n",
    "                    'video_max': frames.max(),\n",
    "                    'video_min': frames.min(),\n",
    "                    'video_median': np.median(frames)\n",
    "                }\n",
    "                return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "            return {\n",
    "                'video_mean': 0,\n",
    "                'video_std': 0,\n",
    "                'video_max': 0,\n",
    "                'video_min': 0,\n",
    "                'video_median': 0\n",
    "            }\n",
    "\n",
    "    def extract_audio_features(self, video_path):\n",
    "        try:\n",
    "            with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "                video = VideoFileClip(video_path)\n",
    "                audio = video.audio\n",
    "                \n",
    "                if audio is not None:\n",
    "                    temp_audio_path = \"temp_audio.wav\"\n",
    "                    audio.write_audiofile(temp_audio_path, verbose=False, logger=None)\n",
    "                    \n",
    "                    y, sr = librosa.load(temp_audio_path)\n",
    "                    \n",
    "                    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "                    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "                    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "                    \n",
    "                    features = {\n",
    "                        'mfcc_mean': mfcc.mean(),\n",
    "                        'mfcc_std': mfcc.std(),\n",
    "                        'spectral_centroid_mean': spectral_centroid.mean(),\n",
    "                        'zero_crossing_rate_mean': zero_crossing_rate.mean()\n",
    "                    }\n",
    "                    \n",
    "                    os.remove(temp_audio_path)\n",
    "                    return features\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio {video_path}: {str(e)}\")\n",
    "            return {\n",
    "                'mfcc_mean': 0,\n",
    "                'mfcc_std': 0,\n",
    "                'spectral_centroid_mean': 0,\n",
    "                'zero_crossing_rate_mean': 0\n",
    "            }\n",
    "\n",
    "    def extract_text_features(self, text):\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.text_model(**inputs)\n",
    "            \n",
    "            sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            \n",
    "            features = {\n",
    "                'text_embedding_mean': sentence_embedding.mean(),\n",
    "                'text_embedding_std': sentence_embedding.std(),\n",
    "                'text_length': len(str(text).split())\n",
    "            }\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {str(e)}\")\n",
    "            return {\n",
    "                'text_embedding_mean': 0,\n",
    "                'text_embedding_std': 0,\n",
    "                'text_length': 0\n",
    "            }\n",
    "\n",
    "    def extract_features(self, df, is_training=True):\n",
    "        all_features = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
    "            features = {}\n",
    "            \n",
    "            # Extract all modality features\n",
    "            video_features = self.extract_video_features(row['video_clip_path'])\n",
    "            audio_features = self.extract_audio_features(row['video_clip_path'])\n",
    "            text_features = self.extract_text_features(row['Utterance'])\n",
    "            \n",
    "            features.update(video_features)\n",
    "            features.update(audio_features)\n",
    "            features.update(text_features)\n",
    "            \n",
    "            # Add metadata features\n",
    "            features['speaker'] = hash(row['Speaker']) % 2**32\n",
    "            features['Sr No.'] = row['Sr No.']\n",
    "            if is_training:\n",
    "                features['Sentiment'] = row['Sentiment']\n",
    "            \n",
    "            all_features.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1000/1000 [21:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training features shape: (1000, 15)\n",
      "\n",
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 100/100 [02:08<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test features shape: (100, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features for training and test sets\n",
    "extractor = MultimodalFeatureExtractor()\n",
    "\n",
    "print(\"Extracting training features...\")\n",
    "train_features = extractor.extract_features(train_df, is_training=True)\n",
    "print(\"\\nTraining features shape:\", train_features.shape)\n",
    "\n",
    "print(\"\\nExtracting test features...\")\n",
    "test_features = extractor.extract_features(test_df, is_training=False)\n",
    "print(\"\\nTest features shape:\", test_features.shape)\n",
    "\n",
    "# Save features to avoid recomputing\n",
    "train_features.to_csv('train_features.csv', index=False)\n",
    "test_features.to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 1.0}\n",
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.36      0.41        61\n",
      "     neutral       0.57      0.86      0.69        97\n",
      "    positive       0.50      0.10      0.16        42\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.51      0.44      0.42       200\n",
      "weighted avg       0.53      0.55      0.49       200\n",
      "\n",
      "\n",
      "Test Set Performance:\n",
      "\n",
      "Submission file created successfully!\n",
      "\n",
      "Sample predictions:\n",
      "   Sr No. Sentiment\n",
      "0      62   neutral\n",
      "1      72   neutral\n",
      "2     112   neutral\n",
      "3     120  negative\n",
      "4     136  negative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Load features if saved previously\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X = train_features.drop(['Sentiment'], axis=1)\n",
    "y = train_features['Sentiment']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Define parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "xgb_model = xgb.XGBClassifier(**best_params, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = xgb_model.predict(X_val_scaled)\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(classification_report(y_val, val_predictions, target_names=label_encoder.classes_))\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = xgb_model.predict(X_test_scaled)\n",
    "test_predictions_labels = label_encoder.inverse_transform(test_predictions)  # Convert back to original labels\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "#print(classification_report(test_df['Sentiment'], test_predictions_labels))\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Sr No.': test_features[\"Sr No.\"],\n",
    "    'Sentiment': test_predictions_labels\n",
    "})\n",
    "\n",
    "# Save the submission\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "\n",
    "# Print sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
